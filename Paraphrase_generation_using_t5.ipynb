{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO3PtVJPK8TdEureFgYtLWl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyanshi-nigam123/Paraphrase_generation_using_t5/blob/main/Paraphrase_generation_using_t5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "brs5vCWeA-dY"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, TrainerCallback\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-WA9ZxtPB4PM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"paws\", \"labeled_final\")\n",
        "\n",
        "\n",
        "def preprocess_paws(dataset, label=1):\n",
        "  df = pd.DataFrame(dataset)\n",
        "  df = df[df['label']==label]\n",
        "\n",
        "  df['input_text'] = \"paraphrase :\" + df['sentence1']\n",
        "  df['target_text'] = df['sentence2']\n",
        "\n",
        "  return df[['input_text','target_text']]\n",
        "\n",
        "train_df = preprocess_paws(dataset['train']).sample(3000, random_state=42)\n",
        "test_df = preprocess_paws(dataset['test']).sample(300, random_state=42)\n",
        "validation_df = preprocess_paws(dataset['validation']).sample(300, random_state=42)"
      ],
      "metadata": {
        "id": "w9isW5hCCAoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "validation_dataset = Dataset.from_pandas(validation_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "model_name = \"t5-base\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    max_length = 512  # T5-base typically uses 512 as default\n",
        "\n",
        "    inputs = tokenizer(examples['input_text'], max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "    targets = tokenizer(examples['target_text'], max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "    inputs['labels'] = targets['input_ids']\n",
        "    return inputs\n",
        "\n",
        "# Tokenize datasets\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "validation_dataset = validation_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Set format for PyTorch\n",
        "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "validation_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "print(f\"✓ Train dataset tokenized: {len(train_dataset)} examples\")\n",
        "print(f\"✓ Validation dataset tokenized: {len(validation_dataset)} examples\")\n",
        "print(f\"✓ Test dataset tokenized: {len(test_dataset)} examples\")"
      ],
      "metadata": {
        "id": "Qo60hVhfCItP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_dir = \"/content/drive/MyDrive/results\"\n",
        "model_dir = \"/content/drive/MyDrive/saved_t5_model\"\n",
        "\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "os.makedirs(model_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "3e5nm6cJCJ1B"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments with smaller batch size\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=results_dir,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=100,\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# SAVE MODEL\n",
        "trainer.save_model(model_dir)\n",
        "tokenizer.save_pretrained(model_dir)\n",
        "print(f\"✅ Model saved to: {model_dir}\")"
      ],
      "metadata": {
        "id": "IEYEKaWACOS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "model_dir = \"/content/drive/MyDrive/saved_t5_model\"\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_dir)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "print(\"✅ Model loaded successfully!\")"
      ],
      "metadata": {
        "id": "jIRby3IvnOY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Load the model and tokenizer from the saved directory\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_dir)"
      ],
      "metadata": {
        "id": "CKyBuk6GCypW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model_dir\n",
        "model_dir = \"/content/drive/MyDrive/saved_t5_model\""
      ],
      "metadata": {
        "id": "qw9CGu-bcojE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Load the model and tokenizer from the saved directory\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_dir)\n",
        "\n",
        "# Set the device (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Define max_length\n",
        "max_length = 512  # Same as training\n",
        "\n",
        "# Preprocessing function for inference\n",
        "def preprocess_input(sentence):\n",
        "    return \"paraphrase: \" + sentence\n",
        "\n",
        "# Generate paraphrases with corrected num_beams and num_return_sequences\n",
        "def generate_paraphrase(input_text, model, tokenizer, max_length=512, num_beams=5, num_return_sequences=4, top_k=100, top_p=0.9, temperature=1.0):\n",
        "    # Preprocess input\n",
        "    input_text = preprocess_input(input_text)\n",
        "\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=max_length, padding=\"max_length\")\n",
        "\n",
        "    # Move inputs to the same device as the model\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "    # Generate paraphrases\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_length=max_length + 20,\n",
        "        num_beams=num_beams,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        temperature=temperature,\n",
        "        do_sample=True,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode generated outputs\n",
        "    paraphrased_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "    return paraphrased_texts\n",
        "\n",
        "# Example sentence\n",
        "input_sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Generate paraphrases\n",
        "paraphrased_sentences = generate_paraphrase(\n",
        "    input_sentence, model, tokenizer, max_length=512, num_return_sequences=4\n",
        ")\n",
        "\n",
        "# Display results\n",
        "print(f\"Original: {input_sentence}\")\n",
        "for i, paraphrase in enumerate(paraphrased_sentences, 1):\n",
        "    print(f\"Paraphrase {i}: {paraphrase}\")"
      ],
      "metadata": {
        "id": "6TyhuX10C31q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = \"She enjoys reading books on rainy afternoons.\"\n",
        "\n",
        "paraphrased_sentences = generate_paraphrase(\n",
        "    input_sentence, model, tokenizer, num_return_sequences=4\n",
        ")\n",
        "\n",
        "print(f\"Original: {input_sentence}\")\n",
        "for i, paraphrase in enumerate(paraphrased_sentences, 1):\n",
        "    print(f\"Paraphrase {i}: {paraphrase}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tO2wC3BoC60M",
        "outputId": "2780abf7-f895-435b-d544-6f5468c149e8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: She enjoys reading books on rainy afternoons.\n",
            "Paraphrase 1: She enjoys reading books on rainy afternoons.\n",
            "Paraphrase 2: She enjoys reading books on rainy afternoons .\n",
            "Paraphrase 3: On rainy afternoons, she enjoys reading books.\n",
            "Paraphrase 4: She loves reading books on rainy afternoons.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = \"The dog barked loudly at the stranger outside the house.\"\n",
        "\n",
        "paraphrased_sentences = generate_paraphrase(\n",
        "    input_sentence, model, tokenizer, num_return_sequences=4\n",
        ")\n",
        "\n",
        "print(f\"Original: {input_sentence}\")\n",
        "for i, paraphrase in enumerate(paraphrased_sentences, 1):\n",
        "    print(f\"Paraphrase {i}: {paraphrase}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6ien9CTC810",
        "outputId": "3e1118ad-8366-4261-b9a5-4312570500dd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: The dog barked loudly at the stranger outside the house.\n",
            "Paraphrase 1: The dog barked loudly at the stranger outside the house.\n",
            "Paraphrase 2: The dog barked loudly at the stranger outside the house .\n",
            "Paraphrase 3: The dog barked loudly at a stranger outside the house.\n",
            "Paraphrase 4: The dog loudly barked at the stranger outside the house.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = \"Climate change is one of the most pressing issues of our time.\"\n",
        "\n",
        "paraphrased_sentences = generate_paraphrase(\n",
        "    input_sentence, model, tokenizer, num_return_sequences=4\n",
        ")\n",
        "\n",
        "print(f\"Original: {input_sentence}\")\n",
        "for i, paraphrase in enumerate(paraphrased_sentences, 1):\n",
        "    print(f\"Paraphrase {i}: {paraphrase}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dzlsu_S2C_8i",
        "outputId": "a20f9acd-ed5e-4d55-afb8-d902e1093869"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: Climate change is one of the most pressing issues of our time.\n",
            "Paraphrase 1: Climate change is one of the most pressing issues of our time.\n",
            "Paraphrase 2: Climate change is one of the most pressing issues of our time .\n",
            "Paraphrase 3: The climate change is one of the most pressing issues of our time.\n",
            "Paraphrase 4: Climate Change is one of the most pressing issues of our time.\n"
          ]
        }
      ]
    }
  ]
}